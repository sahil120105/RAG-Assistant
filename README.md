# RAG Knowledge Assistant

> **Intelligent question-answering system powered by Retrieval-Augmented Generation for custom knowledge bases**

## Description

A streamlined RAG system that provides accurate answers from your custom documents without hallucination. Built with Streamlit for easy web interaction, this tool lets you upload documents or load from URLs, then query your knowledge base using retrieval techniques combined with large language models.

**RAG:** Instead of relying on potentially inaccurate LLM responses, this system first searches your documents for relevant information, then generates contextually grounded answers based on that retrieved content.

## Features

- **ğŸ“ Dual Upload Options**: Upload documents via Streamlit interface or load from web URLs
- **ğŸ” Smart Document Processing**: Automatic text chunking and semantic search with FAISS
- **ğŸ§  Multiple LLM Support**: Llama3 (Ollama) or Groq
- **ğŸ”— Flexible Embeddings**: Hugging Face Sentence Transformers and Ollama embeddings
- **ğŸš« Zero Hallucination**: Responses grounded only in your provided documents
- **âš¡ Local & Cloud Options**: Run locally with Ollama or use cloud APIs
- **ğŸ¯ Stuff Document Chain**: Efficient document concatenation for context building

## How It Works (Architecture/Flow)

```
1. ğŸ“¥ Document Input
   â”œâ”€â”€ Upload files via Streamlit (Q&A_Chatbots/app.py)
   â””â”€â”€ Load from URLs via web interface (groq/app.py)

2. âœ‚ï¸ Document Processing
   â”œâ”€â”€ Split documents into optimized chunks
   â”œâ”€â”€ Apply text cleaning and preprocessing
   â””â”€â”€ Use stuff document chain for context assembly

3. ğŸ”¢ Embedding & Storage
   â”œâ”€â”€ Generate vector embeddings (HF Transformers/Ollama)
   â””â”€â”€ Store in FAISS vector database for fast retrieval

4. ğŸ” Query & Retrieve
   â”œâ”€â”€ Convert user question to embeddings
   â”œâ”€â”€ Perform similarity search in vector store
   â””â”€â”€ Retrieve most relevant document chunks

5. ğŸ¤– Generate Response
   â”œâ”€â”€ Combine retrieved context using stuff chain
   â””â”€â”€ Generate grounded answer with selected LLM
```

## Installation & Setup

### Quick Start

```bash
# Clone repository
git clone https://github.com/yourusername/rag-knowledge-assistant.git
cd rag-knowledge-assistant

# Install dependencies
pip install -r requirements.txt

```

### Environment Variables
Add to your `.env` file:
```env
OPENAI_API_KEY=your_openai_api_key_here
GROQ_API_KEY=your_groq_api_key_here
HUGGINGFACE_API_TOKEN=your_huggingface_token_here
```

### Running the Applications

```bash
cd final_rag
streamlit run app.py
```

### Optional: Local LLM Setup
```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull Llama3 model
ollama pull llama3
```

## Technologies Used

- **ğŸ Python & Streamlit**: Web interface and core logic
- **ğŸ¦™ LangChain**: RAG pipeline and stuff document chains
- **ğŸ¤— Hugging Face**: Sentence transformers for embeddings
- **ğŸ” FAISS**: Vector similarity search and storage
- **ğŸ¦™ Ollama**: Local LLM runtime (Llama3)
- **âš¡ Groq**: High-speed LLM inference
- **ğŸ¤– OpenAI**: GPT model integration
- **ğŸ“„ Document Processing**: Multi-format support with intelligent chunking
